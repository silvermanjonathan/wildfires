{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T22:17:26.359068Z",
     "start_time": "2021-05-05T22:17:26.356527Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wildfires can have many causes, which are determined by expert investigators.  Sometimes, the cause of a fire is obvious: an eyewitness account of arsonists leaving the scene of the crime. However it isn't always an open and shut case nor is it always arson. Other causes can be debris burning, equipment use, lightning, or a campfire gone awry. An investigation is always required. Can machine learning algorithms assist investigators of wildfires?  This is the question I set out to explore. \n",
    "\n",
    "\n",
    "\n",
    "In order to build a model, data is required; the more data the better. Mercifully, thereâ€™s a fairly clean dataset of over a million wildfires found on Kaggle.  The dataset has many variables. However, not all given variables will be used in the model. Furthermore, part of the art of model building entails bringing in extra variables from different datasets.\n",
    "\n",
    "A very reasonable feature to include in the modeling process is weather. Getting the weather for 1.88 million rows of data was the most challenging part of the project for a novice coder like myself. Trial and error on 1.88 million rows of data will take forever to compute. Fortunately, I thought of clever shortcuts which are detailed in the notebook. \n",
    "\n",
    "\n",
    "\n",
    "Modeling is developed iteratively. In other words,  a base model whose measures of veracity are overall improved with sequential modeling decisions, leading to new models.  At the very least, the model should be better than chance. In a binary classification model, accuracy should be better than 50%.  The base model here gives the right answer 61% of the time, and is a simple logistic model without exogenous features.\n",
    "\n",
    "\n",
    "\n",
    "For evaluating our model simplistically, we can look at accuracy. The number of observations correctly classified over the total possible number of observations is accuracy.  This is also called the True Positive rate.  Further metrics to get a comprehensive picture of model performance are detailed in the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T22:25:06.409242Z",
     "start_time": "2021-05-05T22:25:06.407013Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Import Necessary Packages and Modules Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:46:25.688657Z",
     "start_time": "2021-05-05T15:46:25.685236Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import datetime\n",
    "\n",
    "import operator\n",
    "\n",
    "import requests\n",
    "\n",
    "import sklearn.neighbors\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T22:26:38.450491Z",
     "start_time": "2021-05-05T22:26:38.446714Z"
    },
    "hidden": true
   },
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T22:28:10.619370Z",
     "start_time": "2021-05-05T22:28:10.617085Z"
    }
   },
   "source": [
    "# Data Preparation For the Feature of Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T22:27:22.089645Z",
     "start_time": "2021-05-05T22:27:22.086317Z"
    }
   },
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:09:59.238394Z",
     "start_time": "2021-05-06T00:09:59.236256Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## A Function To Get NOAA Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NOAA is the National Oceanic and Atmospheric Administration for the USA. I always thought that was a good acronym for an organization concerned with weather.  NOAA sounds like Noah, the protagonist in the Biblcial Flood story, and a flood is an extreme weather event. But I digress.\n",
    "\n",
    "NOAA has weather stations all across the United States. NOAA also makes its historical weather data free to use. Therefore, NOAA is a natural choice for getting historical weather conditions for a data science project. My objective is to get the historical weather conditions for 1.88 million wildfires from 1992 to 2015. This objective is do-able, but requires breaking it down into smaller tasks. \n",
    "\n",
    "First, I need a list of all the weather stations and their geographic coordinates. However this list wasn't available. I would have to create this list myself.  Fortunately, NOAA lets you download a spreadsheet for the weather on a single day in any state.  It's a request I make on the website and only takes a few minutes to process. When my request is processed, I am sent a link to a website, with the cvs file. This is not an API request. It's manually navigating buttons on a website, done fifty times. \n",
    "\n",
    "Then I created a function that will quickly visit these websites, change the cvs file into a dataframe, extract the relevant data, and put that into a new dataframe. These urls will be defunct after a time period. For ease of reproducibility, the function saves its results in a cvs file on the computer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:05:58.666955Z",
     "start_time": "2021-05-06T00:05:58.658679Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def NOAA_stations(url):\n",
    "    \n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    # Backup csv file from the web\n",
    "    df.to_csv(url[-11:], index=False)\n",
    "\n",
    "    api_station_name = ['GHCND:' + i for i in df.STATION.values]\n",
    "\n",
    "    api_station_state = [df.NAME[0][-5:-3] for i in range(len(df))]\n",
    "    \n",
    "    api_station_latitude = df.LATITUDE\n",
    "\n",
    "    api_station_longitude = df.LONGITUDE\n",
    "\n",
    "    columns = ['name', 'state', 'latitude', 'longitude']\n",
    "\n",
    "    data = [api_station_name, api_station_state,\n",
    "            api_station_latitude, api_station_longitude]\n",
    "\n",
    "    data = dict(zip(columns, data))\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Backup the function's results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NOAA_stations_df = pd.concat(df_list, axis = 0)\n",
    "NOAA_stations_df.to_csv('NOAA_stations_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T23:12:52.943500Z",
     "start_time": "2021-05-05T23:12:52.939236Z"
    },
    "hidden": true
   },
   "source": [
    "-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:08:54.767354Z",
     "start_time": "2021-05-06T00:08:54.765336Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "##   Import NOAA Weather Stations & Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NOAA_stations_df = pd.read_csv('NOAA_stations_df.csv')\n",
    "NOAA_stations_df.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:17:56.847731Z",
     "start_time": "2021-05-06T00:17:56.845738Z"
    }
   },
   "source": [
    "# The magic of rounding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a list of all the NOAA weather stations in the United States, with their precise geographic coordinates. We can now determine which station is closer to which wildfire. This will be done through trial and error. However, without using any shortcuts it will take too long.  A major shortcut is rounding.  \n",
    "\n",
    "In our dataset, the coordinates of wildfires is give to several decimal places. However there are way fewer weather stations than wildfire incidents.  We won't actually lose that much information, if we round the wildfires' coordinates to the ones place.  The number of unique locations drops a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:32:06.043610Z",
     "start_time": "2021-05-06T00:32:06.040465Z"
    }
   },
   "source": [
    "Importing the original dataset of wildfires data from Kaggle. This is a very large dataset.  Therefore, you must download it into your own computer.  Find it on https://www.kaggle.com/rtatman/188-million-us-wildfires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('wildfires.sqlite')\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "df = pd.read_sql_query(\"select * from fires;\", conn)\n",
    "\n",
    "df.to_csv('wildfires.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wildfires.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of observations \" ,len(df))\n",
    "\n",
    "print(\"The number of unique latitudes \" ,len(set(wildfires_coordinates_df.LATITUDE)))\n",
    "\n",
    "rounding_effects_lat = [len(set(wildfires_coordinates_df.LATITUDE.round(i))) for i in range(9)]\n",
    "\n",
    "print(\"How the number of unique latitudes changes with rounding \")\n",
    "print(rounding_effects_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a subset of the original dataset with only the features we need for finding the closest weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfires_coordinates_df = df[['OBJECTID','STATE','LATITUDE','LONGITUDE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfires_coordinates_df['LATITUDE'] = wildfires_coordinates_df.LATITUDE.round()\n",
    "\n",
    "wildfires_coordinates_df['LONGITUDE'] = wildfires_coordinates_df.LONGITUDE.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the latitudes and longitudes are rounded, some geographic coordinates may be repeated. \n",
    "In order to find the unique coordinates, we have to look at latitude and longitude together for each observation.\n",
    "Therefore we will turn them into tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_tups = list(zip(wildfires_coordinates_df.LATITUDE.values ,wildfires_coordinates_df.LONGITUDE.values))\n",
    "\n",
    "wildfires_coordinates_df['LAT_LONG'] = lat_long_tups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:44:26.222773Z",
     "start_time": "2021-05-06T00:44:26.220459Z"
    }
   },
   "source": [
    "# Creating a Dictionary: Wildfire coordinates to OrderIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a dictionary that has the unique tuples and all the objectids assocaited with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [wildfires_coordinates_df[wildfires_coordinates_df.LAT_LONG ==\n",
    "                                   i].OBJECTID.values for i in lat_long_tups_unique]\n",
    "\n",
    "geo_dict = dict(zip(lat_long_tups_unique, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dictionary of OrderIDs to other OrderIDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the code below are all the OrderIDs which share a common geographic coordinate for their wildfire. It follows, that a single orderID can act as a key for all the orderIDs, including itself which have the same wildfire coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_keys = [i[0] for i in values]\n",
    "\n",
    "orderID_group_dict = dict(zip(new_keys,values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:06:43.317915Z",
     "start_time": "2021-05-06T00:06:43.314238Z"
    }
   },
   "source": [
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:59:43.470142Z",
     "start_time": "2021-05-06T00:59:43.468048Z"
    }
   },
   "source": [
    "# Haversian Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the distance between two points on a sphere requires some special mathematics.  Fortunately there are functions already made to calculate such distances, called Haversian distances.  Calculations with Haversian distances are done in radians. Latitude and longitude must be converted into radians. I adapted the code from this resource on Haversian Distances:   https://codeburst.io/calculate-haversine-distance-between-two-geo-locations-with-python-439186315f1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA_stations_df[['lat_radians_Y', 'long_radians_Y']] = (\n",
    "    np.radians(NOAA_stations_df.loc[:, ['latitude', 'longitude']]))\n",
    "\n",
    "\n",
    "Places_X = pd.DataFrame({'OBJECTID': [i[0] for i in values],\n",
    "                         'latitude': [i[0] for i in lat_long_tups_unique],\n",
    "                         'longitude': [i[-1] for i in lat_long_tups_unique]})\n",
    "\n",
    "\n",
    "Places_X[['lat_radians_X', 'long_radians_X']] = np.radians(\n",
    "    Places_X.loc[:, ['latitude', 'longitude']])\n",
    "\n",
    "dist = sklearn.neighbors.DistanceMetric.get_metric('haversine')\n",
    "\n",
    "dist_matrix = (dist.pairwise\n",
    "               (Places_X[['lat_radians_X', 'long_radians_X']],\n",
    "                NOAA_stations_df[['lat_radians_Y', 'long_radians_Y']])*3959\n",
    "               )\n",
    "\n",
    "df_dist_matrix = (\n",
    "    pd.DataFrame(dist_matrix,index=Places_X['OBJECTID'], \n",
    "                 columns=NOAA_stations_df['name'])\n",
    ")\n",
    "\n",
    "\n",
    "df_dist_unpv = (\n",
    "    pd.melt(df_dist_matrix.reset_index(),id_vars='OBJECTID')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = []\n",
    "\n",
    "station_name_dict = {}\n",
    "\n",
    "for k, v in orderID_group_dict.items():\n",
    "\n",
    "   # put a print statement to know something is running\n",
    "    \n",
    "    k_list.append(k)\n",
    "    \n",
    "    count = len(k_list)\n",
    "    \n",
    "    print(count/1232)\n",
    "\n",
    "    subset = df_dist_unpv.OBJECTID == k\n",
    "    \n",
    "    closest_staton_name = df_dist_unpv.loc[(df_dist_unpv['OBJECTID']==k)].sort_values(by = 'value').head(1).name.values[0]\n",
    "\n",
    "    station_name_dict[k] = closest_staton_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:33:19.691101Z",
     "start_time": "2021-05-06T01:33:19.688919Z"
    }
   },
   "source": [
    "# Looking between dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict = {}\n",
    "\n",
    "for k,v in station_name_dict.items():\n",
    "\n",
    "    for key in orderID_group_dict[k]:\n",
    "        \n",
    "        master_dict[key] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_dict = OrderedDict(sorted(master_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:40:28.817161Z",
     "start_time": "2021-05-06T01:40:28.815023Z"
    }
   },
   "source": [
    "# Grand Finale - Making the Closest Weather Station  a Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfires_coordinates_df['NOAA'] = list(master_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:40:20.025714Z",
     "start_time": "2021-05-06T01:40:20.023562Z"
    }
   },
   "source": [
    "# Converting Julian Dates to Standard Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DISCOVERY_DATE'] = pd.to_datetime(df['DISCOVERY_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\n",
    "\n",
    "wildfires_coordinates_df['DISCOVERY_DATE'] = df['DISCOVERY_DATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfires_coordinates_df.to_csv('wildfires_coordinates_df', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:15:57.645436Z",
     "start_time": "2021-05-06T00:15:57.642028Z"
    }
   },
   "source": [
    "# Adding a Feature of Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:47:11.343118Z",
     "start_time": "2021-05-05T12:47:09.651111Z"
    }
   },
   "outputs": [],
   "source": [
    "wildfires_coordinates_df = pd.read_csv('wildfires_coordinates_df')\n",
    "\n",
    "wildfires_coordinates_df['year'] = pd.DatetimeIndex(\n",
    "    wildfires_coordinates_df.DISCOVERY_DATE).year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T16:53:34.582183Z",
     "start_time": "2021-05-04T16:53:34.579767Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# EDA about Causes of Wildfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T16:54:49.010150Z",
     "start_time": "2021-05-04T16:54:48.841813Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debris Burning       0.228150\n",
       "Miscellaneous        0.172194\n",
       "Arson                0.149673\n",
       "Lightning            0.148085\n",
       "Missing/Undefined    0.088661\n",
       "Equipment Use        0.078498\n",
       "Campfire             0.040489\n",
       "Children             0.032528\n",
       "Smoking              0.028115\n",
       "Railroad             0.017791\n",
       "Powerline            0.007683\n",
       "Fireworks            0.006116\n",
       "Structure            0.002019\n",
       "Name: STAT_CAUSE_DESCR, dtype: float64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wildfires_df.STAT_CAUSE_DESCR.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which  and When"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T18:35:28.607028Z",
     "start_time": "2021-05-04T18:35:28.604635Z"
    }
   },
   "source": [
    "## Which State had the most fires and in which year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:47:14.446529Z",
     "start_time": "2021-05-05T12:47:14.020633Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = wildfires_coordinates_df.groupby(['year', 'STATE']).count().OBJECTID\n",
    "\n",
    "unstack_df = groups.unstack()\n",
    "\n",
    "unstack_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:47:14.825546Z",
     "start_time": "2021-05-05T12:47:14.818421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most wildires in a single year was 19453, occurring in TX.\n"
     ]
    }
   ],
   "source": [
    "state_dict = dict(zip(list(unstack_df.columns), [max(\n",
    "    unstack_df[i]) for i in list(unstack_df.columns)]))\n",
    "\n",
    "max_value = max(state_dict.values())\n",
    "\n",
    "max_key = max(state_dict.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "\n",
    "print('The most wildires in a single year was ' +\n",
    "      str(round(max_value)) + ',' + ' occurring in ' + str(max_key) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:48:16.264727Z",
     "start_time": "2021-05-05T12:48:16.147079Z"
    }
   },
   "outputs": [],
   "source": [
    "Texas_df = wildfires_coordinates_df[wildfires_coordinates_df.STATE == 'TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:48:17.396545Z",
     "start_time": "2021-05-05T12:48:17.330525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year with 19453 wildfires was 2011.\n"
     ]
    }
   ],
   "source": [
    "year_dict = dict(zip(Texas_df.groupby(['year']).count(\n",
    ").OBJECTID.index, Texas_df.groupby(['year']).count().OBJECTID.values))\n",
    "\n",
    "max_value = max(year_dict.values())\n",
    "\n",
    "max_key = max(year_dict.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "print('The year with ' + str(max_value) +\n",
    "      ' wildfires was ' + str(max_key) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:49:28.201403Z",
     "start_time": "2021-05-05T12:49:28.186311Z"
    }
   },
   "outputs": [],
   "source": [
    "Texas_2011_df = Texas_df[Texas_df.year == 2011]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T16:47:17.736432Z",
     "start_time": "2021-05-04T16:47:17.733803Z"
    }
   },
   "source": [
    "# Get Features & Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Call for Weather Tempertures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:50:09.327150Z",
     "start_time": "2021-05-05T12:50:09.321549Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_weather(datasetid, stationid, startdate, enddate, datatypeid ,limit, token):\n",
    "\n",
    "    token = {'token': token}\n",
    "\n",
    "    params = 'datasetid=' + datasetid + '&' + 'stationid=' + stationid + '&' + \\\n",
    "        'startdate=' + str(startdate) + '&' + 'enddate=' + \\\n",
    "        str(enddate) + '&' + 'datatypeid=' + datatypeid + '&' + 'limit=' + str(limit)\n",
    "\n",
    "    r = requests.get(base_url, params=params, headers=token)\n",
    "\n",
    "    print(\"Request status code: \"+ str(r.status_code))\n",
    "\n",
    "    return json.loads(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T17:04:21.806425Z",
     "start_time": "2021-05-04T17:04:21.803210Z"
    }
   },
   "source": [
    "## API Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T12:50:12.141553Z",
     "start_time": "2021-05-05T12:50:12.136799Z"
    }
   },
   "outputs": [],
   "source": [
    "station_list = set(Texas_2011_df.NOAA)\n",
    "\n",
    "station_list_dict = dict(enumerate(station_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T13:00:13.115212Z",
     "start_time": "2021-05-05T13:00:00.592367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n",
      "Request status code: 200\n"
     ]
    }
   ],
   "source": [
    "token = 'wtvrEYejcUwYDltcGRmzjkkHkxEhQfor'\n",
    "\n",
    "base_url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data?\"\n",
    "\n",
    "startdate = '2011-01-01'\n",
    "\n",
    "enddate = '2011-12-31'\n",
    "\n",
    "datatypeid = 'TMAX'\n",
    "\n",
    "results = [get_weather('GHCND', i, startdate, enddate, datatypeid , 1000 , token ) for i in station_list] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset dataframe based on available weather station data from the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T13:05:00.761937Z",
     "start_time": "2021-05-05T13:05:00.758517Z"
    }
   },
   "outputs": [],
   "source": [
    "station_result_dict = dict(\n",
    "    enumerate([len(results[i]) != 0 for i in range(len(station_list))]))\n",
    "\n",
    "conditions = []\n",
    "\n",
    "for k, v in station_list_dict.items():\n",
    "    if station_result_dict[k] == True:\n",
    "        conditions.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T13:05:38.623403Z",
     "start_time": "2021-05-05T13:05:38.587257Z"
    }
   },
   "outputs": [],
   "source": [
    "Texas_2011_df_ready = pd.concat([Texas_2011_df[Texas_2011_df.NOAA == i] for i in conditions], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn api results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T13:08:58.580345Z",
     "start_time": "2021-05-05T13:08:58.576331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 9, 12, 13, 16, 17, 18, 20, 22, 23, 28, 29]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = [k for k in station_result_dict.keys() if station_result_dict[k] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T13:09:01.157073Z",
     "start_time": "2021-05-05T13:09:01.113251Z"
    }
   },
   "outputs": [],
   "source": [
    "api_result_df_list = []\n",
    "\n",
    "for i in filtered:\n",
    "\n",
    "    api_result_df = pd.DataFrame(results[i]['results'])\n",
    "\n",
    "    api_result_df.drop('attributes', axis=1, inplace=True)\n",
    "\n",
    "    api_result_df['date'] = pd.to_datetime(\n",
    "        api_result_df['date'], format='%Y-%m-%d')\n",
    "\n",
    "    api_result_df_list.append(api_result_df)\n",
    "\n",
    "api_result_df = pd.concat(api_result_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T02:06:36.783008Z",
     "start_time": "2021-05-06T02:06:36.775338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a nested dictionary to make it easier to enter Temperture data for modeling\n",
    "# first key is noaa station the values are the keys of dates, whose values are the temperture readings.\n",
    "\n",
    "keys = list(api_result_df.station)\n",
    "\n",
    "feature_making_dict = {}\n",
    "\n",
    "for i in keys:\n",
    "\n",
    "    feature_making_dict[i] = ''\n",
    "\n",
    "    pre_dict_df = api_result_df[api_result_df.station ==\n",
    "                                i][['date', 'value']]\n",
    "    dict2 = dict(zip(pre_dict_df.date, pre_dict_df.value))\n",
    "\n",
    "    feature_making_dict[i] = dict2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T15:28:01.245181Z",
     "start_time": "2021-05-04T15:28:01.242821Z"
    }
   },
   "source": [
    "### Creating the Temperture Feature in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to lookup keys in a nested dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T13:10:43.506815Z",
     "start_time": "2021-05-05T13:10:43.503873Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_row(row_number):\n",
    "    return Texas_2011_df_ready[['NOAA', 'DISCOVERY_DATE']].iloc[row_number, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:17:24.401690Z",
     "start_time": "2021-05-05T14:17:13.083759Z"
    }
   },
   "outputs": [],
   "source": [
    "# this gives me the two keys in the nested dictionary...\n",
    "\n",
    "TMAX = [feature_making_dict.get(df_row(i)[0]).get(pd.to_datetime(df_row(i)[1])) for i in range(len(Texas_2011_df_ready))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:18:57.795438Z",
     "start_time": "2021-05-05T14:18:57.791199Z"
    }
   },
   "outputs": [],
   "source": [
    "Texas_2011_df_ready['TMAX'] = TMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:08.853215Z",
     "start_time": "2021-05-05T14:20:08.850889Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = Texas_2011_df_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Fire Cause, Day of the Year Feature, Time of Fire From Original Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:19:18.185238Z",
     "start_time": "2021-05-05T14:19:06.005679Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathansilverman/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (8,10,11,12,13,14,15,16,17,18,35,37) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "wildfires_df = pd.read_csv('wildfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:18.093272Z",
     "start_time": "2021-05-05T14:20:16.774396Z"
    }
   },
   "outputs": [],
   "source": [
    "objectID_needed = df_features.OBJECTID.values\n",
    "\n",
    "fire_cause_dict = dict(zip(wildfires_df.OBJECTID.values, wildfires_df.STAT_CAUSE_DESCR))\n",
    "\n",
    "DOY_dict = dict(zip(wildfires_df.OBJECTID.values, wildfires_df.DISCOVERY_DOY))\n",
    "\n",
    "TIME_dict = dict(zip(wildfires_df.OBJECTID.values, wildfires_df.DISCOVERY_TIME))\n",
    "\n",
    "df_features['STAT_CAUSE_DESCR'] = [fire_cause_dict.get(i) for i in objectID_needed]\n",
    "\n",
    "df_features['DISCOVERY_DOY'] = [DOY_dict.get(i) for i in objectID_needed]\n",
    "\n",
    "df_features['DISCOVERY_TIME'] = [TIME_dict.get(i) for i in objectID_needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:21.204416Z",
     "start_time": "2021-05-05T14:20:21.191115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LAT_LONG</th>\n",
       "      <th>NOAA</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>year</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1459871</th>\n",
       "      <td>1459872</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>2011</td>\n",
       "      <td>294.0</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>144</td>\n",
       "      <td>1436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525270</th>\n",
       "      <td>1525271</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-05-11</td>\n",
       "      <td>2011</td>\n",
       "      <td>272.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>131</td>\n",
       "      <td>1430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525565</th>\n",
       "      <td>1525566</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-06-12</td>\n",
       "      <td>2011</td>\n",
       "      <td>378.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>163</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525932</th>\n",
       "      <td>1525933</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>2011</td>\n",
       "      <td>361.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>193</td>\n",
       "      <td>1735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550976</th>\n",
       "      <td>1550977</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-02-20</td>\n",
       "      <td>2011</td>\n",
       "      <td>183.0</td>\n",
       "      <td>Equipment Use</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         OBJECTID STATE  LATITUDE  LONGITUDE        LAT_LONG  \\\n",
       "1459871   1459872    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1525270   1525271    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1525565   1525566    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1525932   1525933    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1550976   1550977    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "\n",
       "                      NOAA DISCOVERY_DATE  year   TMAX STAT_CAUSE_DESCR  \\\n",
       "1459871  GHCND:USC00297867     2011-05-24  2011  294.0    Miscellaneous   \n",
       "1525270  GHCND:USC00297867     2011-05-11  2011  272.0        Lightning   \n",
       "1525565  GHCND:USC00297867     2011-06-12  2011  378.0        Lightning   \n",
       "1525932  GHCND:USC00297867     2011-07-12  2011  361.0        Lightning   \n",
       "1550976  GHCND:USC00297867     2011-02-20  2011  183.0    Equipment Use   \n",
       "\n",
       "         DISCOVERY_DOY  DISCOVERY_TIME  \n",
       "1459871            144          1436.0  \n",
       "1525270            131          1430.0  \n",
       "1525565            163          2200.0  \n",
       "1525932            193          1735.0  \n",
       "1550976             51             NaN  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:29:38.788351Z",
     "start_time": "2021-05-05T14:29:38.783005Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = df_features[df_features.STAT_CAUSE_DESCR != 'Missing/Undefined']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T16:18:26.805686Z",
     "start_time": "2021-05-04T16:18:26.803209Z"
    }
   },
   "source": [
    "# Logistic Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T16:22:01.733315Z",
     "start_time": "2021-05-04T16:22:01.730834Z"
    }
   },
   "source": [
    "### Class Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:29:49.956373Z",
     "start_time": "2021-05-05T14:29:49.948818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Miscellaneous     0.285734\n",
       "Debris Burning    0.265682\n",
       "Equipment Use     0.163257\n",
       "Lightning         0.086167\n",
       "Powerline         0.078851\n",
       "Arson             0.059748\n",
       "Smoking           0.031161\n",
       "Campfire          0.012058\n",
       "Children          0.011652\n",
       "Railroad          0.005419\n",
       "Fireworks         0.000271\n",
       "Name: STAT_CAUSE_DESCR, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.STAT_CAUSE_DESCR.value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:30:48.994785Z",
     "start_time": "2021-05-05T14:30:48.988366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debris Burning:  0.2656821568893104\n",
      "not Debris Burning:  0.7343178431106896\n"
     ]
    }
   ],
   "source": [
    "print('Debris Burning: ',sum(data_df.STAT_CAUSE_DESCR == 'Debris Burning')/len(data_df))\n",
    "\n",
    "print('not Debris Burning: ',sum(data_df.STAT_CAUSE_DESCR != 'Debris Burning')/len(data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was it debris burning, or was it not debris burning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:36:43.618360Z",
     "start_time": "2021-05-05T14:36:43.609925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-108-eb813113346c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['target'] = [int(i) for i in target.values]\n"
     ]
    }
   ],
   "source": [
    "target = data_df.STAT_CAUSE_DESCR == 'Debris Burning'\n",
    "\n",
    "data_df['target'] = [int(i) for i in target.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:58:10.036310Z",
     "start_time": "2021-05-05T14:58:10.021362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-120-b9ce9b5584a0>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df['month'] = [i.month for i in pd.to_datetime(data_df['DISCOVERY_DATE'].values)]\n"
     ]
    }
   ],
   "source": [
    "data_df['month'] = [i.month for i in pd.to_datetime(data_df['DISCOVERY_DATE'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:58:19.866441Z",
     "start_time": "2021-05-05T14:58:19.852845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LAT_LONG</th>\n",
       "      <th>NOAA</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>year</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>target</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1459871</th>\n",
       "      <td>1459872</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>2011</td>\n",
       "      <td>294.0</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>144</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525270</th>\n",
       "      <td>1525271</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-05-11</td>\n",
       "      <td>2011</td>\n",
       "      <td>272.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>131</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525565</th>\n",
       "      <td>1525566</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-06-12</td>\n",
       "      <td>2011</td>\n",
       "      <td>378.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>163</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525932</th>\n",
       "      <td>1525933</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-07-12</td>\n",
       "      <td>2011</td>\n",
       "      <td>361.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>193</td>\n",
       "      <td>1735.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550976</th>\n",
       "      <td>1550977</td>\n",
       "      <td>TX</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>(35.0, -103.0)</td>\n",
       "      <td>GHCND:USC00297867</td>\n",
       "      <td>2011-02-20</td>\n",
       "      <td>2011</td>\n",
       "      <td>183.0</td>\n",
       "      <td>Equipment Use</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         OBJECTID STATE  LATITUDE  LONGITUDE        LAT_LONG  \\\n",
       "1459871   1459872    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1525270   1525271    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1525565   1525566    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1525932   1525933    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "1550976   1550977    TX      35.0     -103.0  (35.0, -103.0)   \n",
       "\n",
       "                      NOAA DISCOVERY_DATE  year   TMAX STAT_CAUSE_DESCR  \\\n",
       "1459871  GHCND:USC00297867     2011-05-24  2011  294.0    Miscellaneous   \n",
       "1525270  GHCND:USC00297867     2011-05-11  2011  272.0        Lightning   \n",
       "1525565  GHCND:USC00297867     2011-06-12  2011  378.0        Lightning   \n",
       "1525932  GHCND:USC00297867     2011-07-12  2011  361.0        Lightning   \n",
       "1550976  GHCND:USC00297867     2011-02-20  2011  183.0    Equipment Use   \n",
       "\n",
       "         DISCOVERY_DOY  DISCOVERY_TIME  target  month  \n",
       "1459871            144          1436.0       0      5  \n",
       "1525270            131          1430.0       0      5  \n",
       "1525565            163          2200.0       0      6  \n",
       "1525932            193          1735.0       0      7  \n",
       "1550976             51             NaN       0      2  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model (without exogenous features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:38:42.382481Z",
     "start_time": "2021-05-05T15:38:42.380309Z"
    }
   },
   "source": [
    "#### y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:18.688062Z",
     "start_time": "2021-05-05T18:48:18.611397Z"
    }
   },
   "outputs": [],
   "source": [
    "y = data_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:38:22.263191Z",
     "start_time": "2021-05-05T15:38:22.261044Z"
    }
   },
   "source": [
    "#### X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Continous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:19.984716Z",
     "start_time": "2021-05-05T18:48:19.975325Z"
    }
   },
   "outputs": [],
   "source": [
    "continous = ['LATITUDE', 'LONGITUDE', 'DISCOVERY_DOY', 'DISCOVERY_TIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:21.687088Z",
     "start_time": "2021-05-05T18:48:21.607276Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical = ['month']\n",
    "\n",
    "X_categorical = pd.get_dummies(data_df[categorical], prefix='month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:23.116765Z",
     "start_time": "2021-05-05T18:48:23.089944Z"
    }
   },
   "outputs": [],
   "source": [
    "X_continous = data_df[continous].fillna(value=0)\n",
    "\n",
    "for column in X_continous.columns:\n",
    "\n",
    "    # Subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n",
    "\n",
    "    X_continous[column] = (X_continous[column] - min(X_continous[column])) / \\\n",
    "        (max(X_continous[column]) - min(X_continous[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:23.754065Z",
     "start_time": "2021-05-05T18:48:23.751429Z"
    }
   },
   "outputs": [],
   "source": [
    "### Concatenate continous and categorical dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:24.231534Z",
     "start_time": "2021-05-05T18:48:24.223776Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_continous,X_categorical], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:36:06.243625Z",
     "start_time": "2021-05-05T15:36:06.241520Z"
    }
   },
   "source": [
    "##  Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:25.180263Z",
     "start_time": "2021-05-05T18:48:25.126982Z"
    }
   },
   "outputs": [],
   "source": [
    "#deal with imbalances\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:26.316085Z",
     "start_time": "2021-05-05T18:48:26.295003Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "\n",
    "model_log = logreg.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:27.657436Z",
     "start_time": "2021-05-05T18:48:27.646728Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_train = model_log.predict(X_train)\n",
    "\n",
    "y_hat_test = model_log.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:51:37.649718Z",
     "start_time": "2021-05-05T15:51:37.646321Z"
    }
   },
   "source": [
    "## How many times was the classifier correct on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:28.635286Z",
     "start_time": "2021-05-05T18:48:28.622954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3501\n",
      "1    2034\n",
      "Name: target, dtype: int64\n",
      "0    0.63252\n",
      "1    0.36748\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "residuals = np.abs(y_train - y_hat_train)\n",
    "\n",
    "\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:53:06.262872Z",
     "start_time": "2021-05-05T15:53:06.259663Z"
    }
   },
   "source": [
    "## How many times was the classifier correct on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:48:35.309311Z",
     "start_time": "2021-05-05T18:48:35.302356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1142\n",
      "1     704\n",
      "Name: target, dtype: int64\n",
      "0    0.618635\n",
      "1    0.381365\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "residuals = np.abs(y_test - y_hat_test)\n",
    "\n",
    "\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:49:42.424708Z",
     "start_time": "2021-05-05T18:49:42.422168Z"
    }
   },
   "source": [
    "## Model with One Exogenous Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T18:57:40.524968Z",
     "start_time": "2021-05-05T18:57:40.463609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.69449\n",
      "1    0.30551\n",
      "Name: target, dtype: float64\n",
      "\n",
      "\n",
      "0    0.68039\n",
      "1    0.31961\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "continous = ['LATITUDE', 'LONGITUDE',\n",
    "             'DISCOVERY_DOY', 'DISCOVERY_TIME', 'TMAX']\n",
    "\n",
    "X_continous = data_df[continous].fillna(value=0)\n",
    "\n",
    "for column in X_continous.columns:\n",
    "\n",
    "    X_continous[column] = (X_continous[column] - min(X_continous[column])) / \\\n",
    "        (max(X_continous[column]) - min(X_continous[column]))\n",
    "\n",
    "X = pd.concat([X_continous, X_categorical], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "\n",
    "model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_hat_train = model_log.predict(X_train)\n",
    "\n",
    "y_hat_test = model_log.predict(X_test)\n",
    "\n",
    "\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "\n",
    "\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "\n",
    "\n",
    "print(pd.Series(residuals).value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
